{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"megham","text":"<p>A library for working with point clouds and related concepts.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Over the last few years I have written a fair bit of code for both the Simons Observatory and CLASS that involves point clouds. When writing that code I have found myself wanting for a cohesive library that handles all my point cloud related tasks. There are things that exist in bits and pieces (numpy, scipy, sklearn's manifold module, pycpd, etc.) but none of them had all of the features I wanted or were implemented in ways that weren't ideal for my usecase.</p> <p>Megham exists to help me bridge that gap. Currently I am targeting only a small set of features that are relevant to my work:</p> <ul> <li>Fitting transforms between point clouds</li> <li>Point set registration (without known correspondence)</li> <li>Outlier detection</li> <li>Multi dimensional scaling</li> </ul> <p>But other features may exist down the line (and PRs are welcome).</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To install this repository run:</p> <pre><code>pip install megham \n</code></pre> <p>If you will be actively developing the code may want to instead clone this repository and run:</p> <pre><code>pip install -e .\n</code></pre> <p>Documentation can be found here</p>"},{"location":"#contributing","title":"Contributing","text":"<p>All are welcome to contribute to this repository as long as long as the code is relevant. In general contributions other than minor changes should follow the branch/fork -&gt; PR -&gt; merge workflow. If you are going to contribute regularly, contact me to get push access to the repository.</p>"},{"location":"#style-and-standards","title":"Style and Standards","text":"<p>In general contributions should be PEP8 with commits in the conventional commits format. This library follows semantic versioning but the version bumping is done automatically and should usually not have be done manually.</p> <p>In order to make following these rules easier this repository is setup to work with commitizen and pre-commit. It is recommended that you make use of these tools to save time.</p> <p>Docstrings should follow the numpy style. API reference docs are automatically built, but any additional narrative documentation or tutorials should go in the <code>docs</code> folder. This project uses <code>mkdocs</code> to generate documentation.</p>"},{"location":"#tool-setup","title":"Tool Setup","text":"<ol> <li>Install both tools with <code>pip install commitizen pre-commit</code>.</li> <li><code>cd</code> into the <code>megham</code> repository it you aren't already in it.</li> <li>(Optional) Setup <code>commitizen</code> to automatically run when you run <code>git commit</code>. Follow instruction here.</li> <li>Make sure the <code>pre-commit</code> hook is installed by running <code>pre-commit install</code>.</li> </ol>"},{"location":"#example-workflow","title":"Example Workflow","text":"<ol> <li>Make a branch for the edits you want to make.</li> <li>Code.</li> <li>Commit your code with a conventional commit message. <code>cz c</code> gives you a wizard that will do this for you, if you followed Step 3 above then <code>git commit</code> will also do this (but not <code>git commit -m</code>).</li> <li>Repeat steps 2 and 3 until the goal if your branch has been completed.</li> <li>Put in a PR.</li> <li>Once the PR is merged the repo version and tag will update automatically.</li> </ol>"},{"location":"joint_cpd/","title":"Joint Coherent Point Drift","text":""},{"location":"joint_cpd/#background","title":"Background","text":"<p>Coherent Point Drift (CPD) is a point set registration algorithm originally described in Myroneko &amp; Song. This algorithm is an excellent choice for point set registration in cases without known correspondences, outperforming many other state-of-the-art methods. There are many existing implementations of CPD, such as pycpd.</p> <p>One assumption made by CPD is that your point clouds exist in a space where all the axes form a single basis. As a result every degree of freedom allowed by your transformation of choice (affine, rigid, etc.) can be used in the final transformation. This is generally OK in cases where your data is purely spatial, but if your point clouds include a non-spatial axis (ie: color) this can cause issues since transformations that mix the spatial and non-spatial axes can occur. Joint CPD is an extension of the base CPD algorithm that exists to address this.</p>"},{"location":"joint_cpd/#the-joint-cpd-algorithm","title":"The Joint CPD Algorithm","text":"<p>The joint CPD algorithm is a simple extension to CPD First we define the concept of \"dimensional groups\", where every axis in a given dimensional group is transformed together. For a \\(L\\)-dimensional point cloud we can define \\(D\\) dimensional groups, where \\(1 \\leq D \\leq L\\). For convenience we define the following notation:</p> \\[ x_{(d)} \\] <p>Where \\(x\\) is a point cloud, and the \\((d)\\) denotes that we are referring to the \\(d\\)'th dimensional group. Each dimensional group can contain up to \\(L\\) dimensions, but cannot overlap with each other at all.</p> <p>With this now defined we can write down the posterior probabilities of the Gaussian Mixture Model (GMM) components as:</p> \\[ P^{old}(m|x_{n}) = \\frac{\\prod_{d=1}^{D}\\exp\\left(-\\frac{1}{2} \\lVert \\frac{x_{(d)n} - T_{(d)}(y_{(d)m})}{\\sigma_{(d)}} \\rVert ^2 \\right)}{\\sum_{k=1}^{M}\\prod_{d=1}^{D}\\exp\\left(-\\frac{1}{2} \\lVert \\frac{x_{(d)n} - T_{(d)}(y_{(d)k})}{\\sigma_{(d)}} \\rVert ^2 \\right) + \\frac{w}{1-w}\\frac{M}{N}\\prod_{d=1}^{D} \\left(2\\pi\\sigma_{(d)}^2\\right)^{l_{(d)}/2}} \\] <p>Where: </p> <ul> <li>\\(x\\) is the target point cloud</li> <li>\\(y\\) is the source point cloud (the GMM centroids)</li> <li>\\(N\\) is the number of points in \\(x\\)</li> <li>\\(M\\) is the number of points in \\(y\\)</li> <li>\\(D\\) is the number of dimensional groups</li> <li>\\(T_{(d)}\\) is the transformation in the \\(d\\)'th dimensional group</li> <li>\\(\\sigma_{(d)}^2\\) is the variance of the GMM in the \\(d\\)'th dimensional group</li> <li>\\(l_{(d)}\\) is the number of dimensions in the \\(d\\)'th dimensional group</li> <li>\\(w\\) is the weight of the uniform distribution </li> </ul> <p>And we can write down the objective function as:</p> \\[ Q = -\\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n})\\log\\left(P^{new}(m)p^{new}(x_{n}|m)\\right) \\] <p>Because the partial derivatives of \\(Q\\) with respect to the components of \\(T_{(d)}\\) and \\(\\sigma_{(d)}\\) kill all terms pertaining to other dimensional groups (see Proof of Equivalence) we can update the transformation and variance as in the original CPD algorithm within each dimensional group.  This allows us to compute \\(D\\) independent transformations, but because all of the dimensional groups are used to compute \\(P^{old}\\) these transformations can leverage correlations between different dimensional groups.</p>"},{"location":"joint_cpd/#proof-of-equivalence","title":"Proof of Equivalence","text":"<p>Starting from the objective function \\(Q\\) as defined above we show that the partial derivatives within each dimensional group reduce to a form that is trivially equivalent to those in the base CPD algorithm below.</p> <p>First we expand \\(Q\\) as:</p> \\[ Q = -\\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n})\\log\\left(\\prod_{d=1}^{D} \\frac{\\exp\\left(-\\frac{\\lVert x_{(d)n} - T_{(d)}(y_{(d)m})\\rVert ^2}{2\\sigma_{(d)}^2} \\right)}{\\left(2\\pi\\sigma_{(d)}^2\\right)^{1/2}}\\right) \\] \\[ Q = -\\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n})\\sum_{d=1}^{D} -\\frac{\\lVert x_{(d)n} - T_{(d)}(y_{(d)m})\\rVert ^2}{2\\sigma_{(d)}^2} - \\frac{\\log\\left(2\\pi\\sigma_{(d)}^2\\right)}{2} \\] \\[ Q = \\sum_{d=1}^{D}\\frac{N_{P}l_{(d)}}{2}\\log\\sigma_{(d)}^2 + \\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n})\\sum_{d=1}^{D} \\frac{\\lVert x_{(d)n} - T_{(d)}(y_{(d)m})\\rVert ^2}{2\\sigma_{(d)}^2} \\] <p>Where \\(N_{P}\\) is the sum of \\(P^{old}\\).</p> <p>Now we take the partial derivatives. First WLOG take the derivative with respect to \\(A\\) where \\(A\\) is a component of the transform (ie: the affine matrix \\(B\\) in the affine case, the rotation matrix \\(R\\) in the rigid case, etc.):</p> \\[ \\frac{dQ}{dA_{(d)}} = \\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n})\\frac{\\lVert x_{(d)n} - \\frac{dT_{(d)}}{dA_{(d)}}(y_{(d)m})\\rVert ^2}{2\\sigma_{(d)}^2} \\] <p>Which is trivially equivalent to the derivative in the base CPD case.</p> <p>Similarly taking the derivative with respect to \\(\\sigma^2\\):</p> \\[ \\frac{dQ}{\\sigma_{(d)}^2} = \\frac{N_{P}l_{(d)}}{2\\sigma_{(d)}^2} - \\sum_{n=1}^{N}\\sum_{m=1}^{M} P^{old}(m|x_{n}) \\frac{\\lVert x_{(d)n} - T_{(d)}(y_{(d)m})\\rVert ^2}{2\\sigma_{(d)}^4} \\] <p>Which is also trivially equivalent to the derivative in the base CPD case.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mds</li> <li>registration<ul> <li>cpd</li> </ul> </li> <li>transform</li> <li>utils</li> </ul>"},{"location":"reference/mds/","title":"mds","text":"<p>Functions for doing multidimensional scaling.</p>"},{"location":"reference/mds/#megham.mds.classic_mds","title":"<code>classic_mds(distance_matrix, ndim=3)</code>","text":"<p>Perform classical (Torgerson) MDS. This assumes a complete euclidean distance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>distance_matrix</code> <code>NDArray[floating]</code> <p>The euclidean distance matrix. Should be (npoint, npoint) and complete (no missing distances).</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>coords</code> <code>NDArray[floating]</code> <p>The output coordinates, will be (npoint, ndim). Points are in the same order as distance_matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance_matrix is not square. If distance_matrix has non-finite values.</p> Source code in <code>megham/mds.py</code> <pre><code>def classic_mds(\n    distance_matrix: NDArray[np.floating], ndim: int = 3\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Perform classical (Torgerson) MDS. This assumes a complete euclidean distance matrix.\n\n    Parameters\n    ----------\n    distance_matrix : NDArray[np.floating]\n        The euclidean distance matrix.\n        Should be (npoint, npoint) and complete (no missing distances).\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n\n    Returns\n    -------\n    coords : NDArray[np.floating]\n        The output coordinates, will be (npoint, ndim).\n        Points are in the same order as distance_matrix.\n\n    Raises\n    ------\n    ValueError\n        If distance_matrix is not square.\n        If distance_matrix has non-finite values.\n    \"\"\"\n    npoint = len(distance_matrix)\n    if distance_matrix.shape != (npoint, npoint):\n        raise ValueError(\"Distance matrix should be square\")\n    if not np.all(np.isfinite(distance_matrix)):\n        raise ValueError(\"Distance matrix must only have finite values\")\n\n    d_sq = distance_matrix**2\n    cent_mat = np.eye(npoint) - np.ones_like(distance_matrix) / npoint\n    double_cent = -0.5 * cent_mat @ d_sq @ cent_mat\n\n    eigen_vals, eigen_vecs = np.linalg.eig(double_cent)\n    eigen_vals = eigen_vals[-1 * ndim :]\n    eigen_vecs = eigen_vecs[:, -1 * ndim :]\n    tol = 1e16\n    eigen_vals[(eigen_vals &lt; 0) &amp; (eigen_vals &gt; -tol)] = 0.0\n\n    coords = eigen_vecs @ (np.diag(np.sqrt(eigen_vals)))\n    return np.real(coords)\n</code></pre>"},{"location":"reference/mds/#megham.mds.metric_mds","title":"<code>metric_mds(distance_matrix, ndim=3, weights=None, guess=None, use_smacof=True, **kwargs)</code>","text":"<p>Perform metric MDS. This is useful over classical MDS if you have missing values or need to weight distances.</p> <p>Parameters:</p> Name Type Description Default <code>distance_matrix</code> <code>NDArray[floating]</code> <p>The distance matrix. Should be (npoint, npoint), unknown distances should be set to nan.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <code>weights</code> <code>Optional[NDArray[floating]]</code> <p>How much to weigh each distance in distance_matrix in the metric. Weights should be finite and non-negative, invalid weights will be set to 0. Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.</p> <code>None</code> <code>guess</code> <code>Optional[NDArray[floating]]</code> <p>Initial guess at coordinates. Should be (npoint, ndim) and in the same order as distance_matrix.</p> <code>None</code> <code>use_smacof</code> <code>bool</code> <p>If True use smacof for the optimization. If False use scipy.optimize.minimize.</p> <code>True</code> <code>**kwargs</code> <p>Keyword arguments to pass to smacof or scipy.optimize.minimize.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>coords</code> <code>NDArray[floating]</code> <p>The output coordinates, will be (npoint, ndim). Points are in the same order as distance_matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance_matrix is not square. If the shape of weights or guess is not consistant with distance_matrix.</p> Source code in <code>megham/mds.py</code> <pre><code>def metric_mds(\n    distance_matrix: NDArray[np.floating],\n    ndim: int = 3,\n    weights: Optional[NDArray[np.floating]] = None,\n    guess: Optional[NDArray[np.floating]] = None,\n    use_smacof: bool = True,\n    **kwargs,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Perform metric MDS.\n    This is useful over classical MDS if you have missing values or need to weight distances.\n\n    Parameters\n    ----------\n    distance_matrix : NDArray[np.floating]\n        The distance matrix.\n        Should be (npoint, npoint), unknown distances should be set to nan.\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n    weights : Optional[NDArray[np.floating]], default: None\n        How much to weigh each distance in distance_matrix in the metric.\n        Weights should be finite and non-negative, invalid weights will be set to 0.\n        Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.\n    guess : Optional[NDArray[np.floating]], default: None\n        Initial guess at coordinates.\n        Should be (npoint, ndim) and in the same order as distance_matrix.\n    use_smacof : bool, default: True\n        If True use smacof for the optimization.\n        If False use scipy.optimize.minimize.\n    **kwargs\n        Keyword arguments to pass to smacof or scipy.optimize.minimize.\n\n    Returns\n    -------\n    coords : NDArray[np.floating]\n        The output coordinates, will be (npoint, ndim).\n        Points are in the same order as distance_matrix.\n\n    Raises\n    ------\n    ValueError\n        If distance_matrix is not square.\n        If the shape of weights or guess is not consistant with distance_matrix.\n    \"\"\"\n    npoint = len(distance_matrix)\n    if distance_matrix.shape != (npoint, npoint):\n        raise ValueError(\"Distance matrix should be square\")\n\n    if weights is None:\n        weights = np.ones_like(distance_matrix)\n    elif weights.shape != (npoint, npoint):\n        raise ValueError(\"Weights must match distance_matrix\")\n    else:\n        neg_msk = weights &lt; 0\n        if np.any(neg_msk):\n            logger.warn(\"Negetive weight found, setting to 0.\")\n            weights[neg_msk] = 0\n        nfin_msk = ~np.isfinite(weights)\n        if np.any(nfin_msk):\n            logger.warn(\"Non-finite weight found, setting to 0.\")\n            weights[nfin_msk] = 0\n\n    if guess is None:\n        logger.info(\"No initial guess provided, using a random set of points.\")\n        guess = _init_coords(npoint, ndim, distance_matrix)\n    elif guess.shape != (npoint, ndim):\n        raise ValueError(\"Guess must be (npoint, ndim)\")\n\n    if use_smacof:\n        coords, _ = smacof(\n            metric_stress, guess, distance_matrix, weights, ndim, **kwargs\n        )\n    else:\n        res = opt.minimize(\n            metric_stress,\n            guess.ravel(),\n            args=(distance_matrix.astype(float), weights.astype(float), ndim),\n            **kwargs,\n        )\n        logger.info(\n            \"Finished with a final stress of %f\\n Optimizer message: %s\",\n            res.fun,\n            res.message,\n        )\n        coords = res.x.reshape((npoint, ndim))\n    return coords\n</code></pre>"},{"location":"reference/mds/#megham.mds.metric_stress","title":"<code>metric_stress(coords, distance_matrix, weights, ndim)</code>","text":"<p>Stress that is minimized for metric MDS.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>NDArray[floating]</code> <p>The coordinates to calculate stress at. Should be (npoint, ndim)</p> required <code>distance_matrix</code> <code>NDArray[floating]</code> <p>The distance matrix. Should be (npoint, npoint), unknown distances should be set to nan.</p> required <code>weights</code> <code>NDArray[floating]</code> <p>How much to weigh each distance in distance_matrix in the metric. Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>stress</code> <code>float</code> <p>The stress of the system at the with the given coordinates.</p> Source code in <code>megham/mds.py</code> <pre><code>def metric_stress(\n    coords: NDArray[np.floating],\n    distance_matrix: NDArray[np.floating],\n    weights: NDArray[np.floating],\n    ndim: int,\n) -&gt; float:\n    \"\"\"\n    Stress that is minimized for metric MDS.\n\n    Parameters\n    ----------\n    coords : NDArray[np.floating]\n        The coordinates to calculate stress at.\n        Should be (npoint, ndim)\n    distance_matrix : NDArray[np.floating]\n        The distance matrix.\n        Should be (npoint, npoint), unknown distances should be set to nan.\n    weights : NDArray[np.floating]\n        How much to weigh each distance in distance_matrix in the metric.\n        Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n\n    Returns\n    -------\n    stress : float\n        The stress of the system at the with the given coordinates.\n    \"\"\"\n    npoint = len(distance_matrix)\n    idx = np.triu_indices(npoint, 1)\n    edm = make_edm(coords.reshape((npoint, ndim)))\n    stress = np.sqrt(np.nansum(weights[idx] * (distance_matrix[idx] - edm[idx]) ** 2))\n    return stress\n</code></pre>"},{"location":"reference/mds/#megham.mds.nonmetric_mds","title":"<code>nonmetric_mds(distance_matrix, ndim=3, weights=None, guess=None, epsilon_outer=1e-10, max_iters_outer=200, use_smacof=False, **kwargs)</code>","text":"<p>Perform nonmetric MDS. This is useful over metric MDS if you have some wierd dissimilarity (ie. not euclidean).</p> <p>Parameters:</p> Name Type Description Default <code>distance_matrix</code> <code>NDArray[floating]</code> <p>The distance matrix. Should be (npoint, npoint), unknown distances should be set to nan.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <code>weights</code> <code>Optional[NDArray[floating]]</code> <p>How much to weigh each distance in distance_matrix in the metric. Weights should be finite and non-negative, invalid weights will be set to 0. Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.</p> <code>None</code> <code>guess</code> <code>Optional[NDArray[floating]]</code> <p>Initial guess at coordinates. Should be (npoint, ndim) and in the same order as distance_matrix.</p> <code>None</code> <code>epsilon_outer</code> <code>float</code> <p>The difference in stress between iterations before stopping outer optimization.</p> <code>1e-10</code> <code>max_iters_outer</code> <code>int</code> <p>Maximum number of iterations for outer optimization.</p> <code>200</code> <code>use_smacof</code> <code>bool</code> <p>If True use smacof for the optimization. If False use scipy.optimize.minimize.</p> <code>False</code> <code>**kwargs</code> <p>Keyword arguments to pass to smacof or scipy.optimize.minimize.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>coords</code> <code>NDArray[floating]</code> <p>The output coordinates, will be (npoint, ndim). Points are in the same order as distance_matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If distance_matrix is not square. If the shape of weights or guess is not consistant with distance_matrix.</p> Source code in <code>megham/mds.py</code> <pre><code>def nonmetric_mds(\n    distance_matrix: NDArray[np.floating],\n    ndim: int = 3,\n    weights: Optional[NDArray[np.floating]] = None,\n    guess: Optional[NDArray[np.floating]] = None,\n    epsilon_outer: float = 1e-10,\n    max_iters_outer: int = 200,\n    use_smacof: bool = False,\n    **kwargs,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Perform nonmetric MDS.\n    This is useful over metric MDS if you have some wierd dissimilarity\n    (ie. not euclidean).\n\n    Parameters\n    ----------\n    distance_matrix : NDArray[np.floating]\n        The distance matrix.\n        Should be (npoint, npoint), unknown distances should be set to nan.\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n    weights : Optional[NDArray[np.floating]], default: None\n        How much to weigh each distance in distance_matrix in the metric.\n        Weights should be finite and non-negative, invalid weights will be set to 0.\n        Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.\n    guess : Optional[NDArray[np.floating]], default: None\n        Initial guess at coordinates.\n        Should be (npoint, ndim) and in the same order as distance_matrix.\n    epsilon_outer : float, default: 1e-10\n        The difference in stress between iterations before stopping outer optimization.\n    max_iters_outer : int, default: 200\n        Maximum number of iterations for outer optimization.\n    use_smacof : bool, default: False\n        If True use smacof for the optimization.\n        If False use scipy.optimize.minimize.\n    **kwargs\n        Keyword arguments to pass to smacof or scipy.optimize.minimize.\n\n    Returns\n    -------\n    coords : NDArray[np.floating]\n        The output coordinates, will be (npoint, ndim).\n        Points are in the same order as distance_matrix.\n\n    Raises\n    ------\n    ValueError\n        If distance_matrix is not square.\n        If the shape of weights or guess is not consistant with distance_matrix.\n    \"\"\"\n    if use_smacof:\n        logger.warn(\n            \"You are using SMACOF with nonmetric mds, this doesn't currently work very well...\"\n        )\n        if \"verbose\" not in kwargs:\n            kwargs[\"verbose\"] = False\n    npoint = len(distance_matrix)\n    if distance_matrix.shape != (npoint, npoint):\n        raise ValueError(\"Distance matrix should be square\")\n\n    if weights is None:\n        weights = np.ones_like(distance_matrix)\n    elif weights.shape != (npoint, npoint):\n        raise ValueError(\"Weights must match distance_matrix\")\n    else:\n        neg_msk = weights &lt; 0\n        if np.any(neg_msk):\n            logger.warn(\"Negetive weight found, setting to 0.\")\n            weights[neg_msk] = 0\n        nfin_msk = not np.isfinite(weights)\n        if np.any(nfin_msk):\n            logger.warn(\"Non-finite weight found, setting to 0.\")\n            weights[nfin_msk] = 0\n\n    if guess is None:\n        logger.info(\"No initial guess provided, using a random set of points.\")\n        guess = _init_coords(npoint, ndim, distance_matrix)\n    elif guess.shape != (npoint, ndim):\n        raise ValueError(\"Guess must be (npoint, ndim)\")\n\n    idx = np.triu_indices(npoint, 1)\n    flat_dist = np.ravel(distance_matrix[idx])\n    flat_weight = np.ravel(weights[idx])\n    msk = np.isfinite(flat_dist) + np.isfinite(flat_weight)\n    flat_dist = flat_dist[msk]\n    flat_weight = flat_weight[msk]\n    f_dist = np.zeros_like(distance_matrix)\n\n    stress = np.inf\n    i = 0\n    ir = IsotonicRegression()\n    coords = guess.copy()\n    for i in range(max_iters_outer):\n        _stress = stress\n        # Make a guess at f\n        edm = make_edm(guess)[idx][msk]\n        _f_dist_flat = ir.fit_transform(flat_dist, edm, sample_weight=flat_weight)\n        f_dist_flat = np.nan + np.empty(msk.shape)\n        f_dist_flat[msk] = _f_dist_flat\n        f_dist[idx] = f_dist_flat\n\n        # Solve for the coordinates at this f\n        if use_smacof:\n            guess, stress = smacof(\n                nonmetric_stress, coords, f_dist, weights, ndim, **kwargs\n            )\n        else:\n            res = opt.minimize(\n                nonmetric_stress,\n                coords.ravel(),\n                args=(f_dist.astype(float), weights.astype(float), ndim),\n                **kwargs,\n            )\n            guess = res.x.reshape((npoint, ndim))\n            stress = res.fun\n        if guess is None:\n            raise RuntimeError(\"Current guess is None, something went wrong...\")\n        coords = guess\n        if _stress - stress &lt; epsilon_outer:\n            break\n        if stress == 0:\n            break\n    logger.info(\"Took %d iterations with a final stress of %f\", i + 1, stress)\n\n    return coords\n</code></pre>"},{"location":"reference/mds/#megham.mds.nonmetric_stress","title":"<code>nonmetric_stress(coords, f_dist, weights, ndim)</code>","text":"<p>Stress that is minimized for nonmetric MDS.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>NDArray[floating]</code> <p>The coordinates to calculate stress at. Should be (npoint, ndim)</p> required <code>f_dist</code> <code>NDArray[floating]</code> <p>The distance matrix with the isotonic regression applied. Should be (npoint, npoint), unknown distances should be set to nan.</p> required <code>weights</code> <code>NDArray[floating]</code> <p>How much to weigh each distance in distance_matrix in the metric. Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>stress</code> <code>float</code> <p>The stress of the system at the with the given coordinates.</p> Source code in <code>megham/mds.py</code> <pre><code>def nonmetric_stress(\n    coords: NDArray[np.floating],\n    f_dist: NDArray[np.floating],\n    weights: NDArray[np.floating],\n    ndim: int,\n) -&gt; float:\n    \"\"\"\n    Stress that is minimized for nonmetric MDS.\n\n    Parameters\n    ----------\n    coords : NDArray[np.floating]\n        The coordinates to calculate stress at.\n        Should be (npoint, ndim)\n    f_dist : NDArray[np.floating]\n        The distance matrix with the isotonic regression applied.\n        Should be (npoint, npoint), unknown distances should be set to nan.\n    weights : NDArray[np.floating]\n        How much to weigh each distance in distance_matrix in the metric.\n        Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n\n    Returns\n    -------\n    stress : float\n        The stress of the system at the with the given coordinates.\n    \"\"\"\n    npoint = len(f_dist)\n    idx = np.triu_indices(npoint, 1)\n    edm = make_edm(coords.reshape((npoint, ndim)))\n\n    num = np.nansum(weights[idx] * (f_dist[idx] - edm[idx]) ** 2)\n    denom = np.nansum(edm[idx] ** 2)\n    stress = np.sqrt(num / denom)\n\n    return stress\n</code></pre>"},{"location":"reference/mds/#megham.mds.smacof","title":"<code>smacof(stress_func, coords, distance_matrix, weights, ndim, max_iters=10000, epsilon=1e-10, verbose=True)</code>","text":"<p>SMACOF algorithm for multidimensional scaling.</p> <p>Parameters:</p> Name Type Description Default <code>stress_func</code> <code>Callable[[NDArray[floating], NDArray[floating], NDArray[floating], int], float]</code> <p>The stress function to use.</p> required <code>coords</code> <code>NDArray[floating]</code> <p>Initial guess of coordinates to calculate stress at. Should be (npoint, ndim)</p> required <code>distance_matrix</code> <code>NDArray[floating]</code> <p>The distance matrix. Should be (npoint, npoint), unknown distances should be set to nan.</p> required <code>weights</code> <code>NDArray[floating]</code> <p>How much to weigh each distance in distance_matrix in the metric. Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.</p> required <code>ndim</code> <code>int</code> <p>The number of dimensions to scale to.</p> <code>3</code> <code>epsilon</code> <code>float</code> <p>The difference in stress between iterations before stopping.</p> <code>1e-10</code> <code>max_iters</code> <code>int</code> <p>The maximum iterations to run for.</p> <code>10000</code> <code>verbose</code> <code>bool</code> <p>Sets the verbosity of this function. If True logs at the INFO level. If False logs at the DEBUG level.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>coords</code> <code>NDArray[floating]</code> <p>The coordinates as optimized by SMACOF.</p> <code>stress</code> <code>float</code> <p>The stress of the system at the final iteration.</p> Source code in <code>megham/mds.py</code> <pre><code>def smacof(\n    stress_func: Callable[\n        [NDArray[np.floating], NDArray[np.floating], NDArray[np.floating], int], float\n    ],\n    coords: NDArray[np.floating],\n    distance_matrix: NDArray[np.floating],\n    weights: NDArray[np.floating],\n    ndim: int,\n    max_iters: int = 10000,\n    epsilon: float = 1e-10,\n    verbose: bool = True,\n) -&gt; tuple[NDArray[np.floating], float]:\n    \"\"\"\n    SMACOF algorithm for multidimensional scaling.\n\n    Parameters\n    ----------\n    stress_func : Callable[[NDArray[np.floating], NDArray[np.floating], NDArray[np.floating], int], float]\n        The stress function to use.\n    coords : NDArray[np.floating]\n        Initial guess of coordinates to calculate stress at.\n        Should be (npoint, ndim)\n    distance_matrix : NDArray[np.floating]\n        The distance matrix.\n        Should be (npoint, npoint), unknown distances should be set to nan.\n    weights : NDArray[np.floating]\n        How much to weigh each distance in distance_matrix in the metric.\n        Should be (npoint, npoint) and have 1-to-1 correspondance with distance_matrix.\n    ndim : int, default: 3\n        The number of dimensions to scale to.\n    epsilon : float, default: 1e-10\n        The difference in stress between iterations before stopping.\n    max_iters : int, default: 10000\n        The maximum iterations to run for.\n    verbose : bool, default: True\n        Sets the verbosity of this function.\n        If True logs at the INFO level.\n        If False logs at the DEBUG level.\n\n    Returns\n    -------\n    coords : NDArray[np.floating]\n        The coordinates as optimized by SMACOF.\n    stress : float\n        The stress of the system at the final iteration.\n    \"\"\"\n    if verbose:\n        log = logger.info\n    else:\n        log = logger.debug\n    i = 0\n    npoint = len(distance_matrix)\n    stress = stress_func(coords, distance_matrix, weights, ndim)\n    for i in range(max_iters):\n        if stress == 0:\n            break\n        _stress = stress\n\n        edm = make_edm(coords)\n\n        B = -1 * weights * distance_matrix / edm\n        B[edm == 0] = 0\n        B[~np.isfinite(B)] = 0\n        B[np.diag_indices_from(B)] -= np.sum(B, axis=1)\n\n        guess = np.dot(B, coords) / npoint\n\n        stress = stress_func(guess, distance_matrix, weights, ndim)\n        coords = guess\n        if _stress - stress &lt; epsilon:\n            break\n    log(\"SMACOF took %d iterations with a final stress of %f\", i + 1, stress)\n    return coords, stress\n</code></pre>"},{"location":"reference/transform/","title":"transform","text":"<p>Functions to computing and working with transformations between point clouds</p>"},{"location":"reference/transform/#megham.transform.apply_transform","title":"<code>apply_transform(src, transform, shift)</code>","text":"<p>Apply a transformation to a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>The points to transform. Should have shape  (npoints, ndim).</p> required <code>transform</code> <code>NDArray[floating]</code> <p>The transformation matrix. Should have shape (ndim, ndim).</p> required <code>shift</code> <code>NDArray[floating]</code> <p>The shift to apply after the affine tranrform. Should have shape (ndim,).</p> required <p>Returns:</p> Name Type Description <code>transformed</code> <code>NDArray[floating]</code> <p>The transformed points. Has the same shape as src.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If src is not a 2d array. If one of src's axis is not of size ndim. If affine and shift have inconsistent shapes.</p> Source code in <code>megham/transform.py</code> <pre><code>def apply_transform(\n    src: NDArray[np.floating],\n    transform: NDArray[np.floating],\n    shift: NDArray[np.floating],\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Apply a transformation to a set of points.\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        The points to transform.\n        Should have shape  (npoints, ndim).\n    transform: NDArray[np.floating]\n        The transformation matrix.\n        Should have shape (ndim, ndim).\n    shift : NDArray[np.floating]\n        The shift to apply after the affine tranrform.\n        Should have shape (ndim,).\n\n    Returns\n    -------\n    transformed : NDArray[np.floating]\n        The transformed points.\n        Has the same shape as src.\n\n    Raises\n    ------\n    ValueError\n        If src is not a 2d array.\n        If one of src's axis is not of size ndim.\n        If affine and shift have inconsistent shapes.\n    \"\"\"\n    ndim = len(shift)\n    if transform.shape != (ndim, ndim):\n        raise ValueError(\n            f\"From shift we assume ndim={ndim} but transform has shape {transform.shape}\"\n        )\n    src_shape = np.array(src.shape)\n    if len(src_shape) != 2:\n        raise ValueError(f\"src should be a 2d array, not {len(src.shape)}d\")\n\n    transformed = src @ transform + shift\n    return transformed\n</code></pre>"},{"location":"reference/transform/#megham.transform.compose_transform","title":"<code>compose_transform(transform_1, shift_1, transform_2, shift_2)</code>","text":"<p>Combine transformations to get one that is equivalent to: dst = (src@transform_1 + shift)@transform_2 + shift_2</p> <p>Parameters:</p> Name Type Description Default <code>transform_1</code> <code>NDArray[floating]</code> <p>The first transform (affine or rotation matrix). Should have shape (ndim, ndim).</p> required <code>shift_1</code> <code>NDArray[floating]</code> <p>The first shift. Should have shape (ndim,).</p> required <code>transform_2</code> <code>NDArray[floating]</code> <p>The second transform (affine or rotation matrix). Should have shape (ndim, ndim).</p> required <code>shift_2</code> <code>NDArray[floating]</code> <p>The second shift. Should have shape (ndim,).</p> required <p>Returns:</p> Name Type Description <code>transform</code> <code>NDArray[floating]</code> <p>The composed transform. Has shape (ndim, ndim).</p> <code>shift</code> <code>NDArray[np.floating].</code> <p>The composed shift. Has shape (ndim,).</p> Source code in <code>megham/transform.py</code> <pre><code>def compose_transform(\n    transform_1: NDArray[np.floating],\n    shift_1: NDArray[np.floating],\n    transform_2: NDArray[np.floating],\n    shift_2: NDArray[np.floating],\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Combine transformations to get one that is equivalent to:\n    dst = (src@transform_1 + shift)@transform_2 + shift_2\n\n    Parameters\n    ----------\n    transform_1 : NDArray[np.floating]\n        The first transform (affine or rotation matrix).\n        Should have shape (ndim, ndim).\n    shift_1 : NDArray[np.floating]\n        The first shift.\n        Should have shape (ndim,).\n    transform_2 : NDArray[np.floating]\n        The second transform (affine or rotation matrix).\n        Should have shape (ndim, ndim).\n    shift_2 : NDArray[np.floating]\n        The second shift.\n        Should have shape (ndim,).\n\n    Returns\n    -------\n    transform : NDArray[np.floating]\n        The composed transform.\n        Has shape (ndim, ndim).\n    shift : NDArray[np.floating].\n        The composed shift.\n        Has shape (ndim,).\n    \"\"\"\n    transform = transform_1 @ transform_2\n    shift = shift_1 @ transform_2 + shift_2\n\n    return transform, shift\n</code></pre>"},{"location":"reference/transform/#megham.transform.decompose_affine","title":"<code>decompose_affine(affine)</code>","text":"<p>Decompose an affine transformation into its components. This decomposetion treats the affine matrix as: rotation * shear * scale.</p> <p>Parameters:</p> Name Type Description Default <code>affine</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) affine transformation matrix.</p> required <p>Returns:</p> Name Type Description <code>scale</code> <code>NDArray[floating]</code> <p>The (ndim,) array of scale parameters.</p> <code>shear</code> <code>NDArray[floating]</code> <p>The (ndim*(ndim - 1)/2,) array of shear parameters.</p> <code>rot</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) rotation matrix. If ndim is 2 or 3 then decompose_rotation can be used to get euler angles.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If affine is not ndim by ndim.</p> Source code in <code>megham/transform.py</code> <pre><code>def decompose_affine(\n    affine: NDArray[np.floating],\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Decompose an affine transformation into its components.\n    This decomposetion treats the affine matrix as: rotation * shear * scale.\n\n    Parameters\n    ----------\n    affine : NDArray[np.floating]\n        The (ndim, ndim) affine transformation matrix.\n\n    Returns\n    -------\n    scale : NDArray[np.floating]\n        The (ndim,) array of scale parameters.\n    shear : NDArray[np.floating]\n        The (ndim*(ndim - 1)/2,) array of shear parameters.\n    rot: NDArray[np.floating]\n        The (ndim, ndim) rotation matrix.\n        If ndim is 2 or 3 then decompose_rotation can be used to get euler angles.\n\n    Raises\n    ------\n    ValueError\n        If affine is not ndim by ndim.\n    \"\"\"\n    ndim = len(affine)\n    if affine.shape != (ndim, ndim):\n        raise ValueError(\"Affine matrix should be ndim by ndim\")\n    # Use the fact that rotation matrix times its transpose is the identity\n    no_rot = affine.T @ affine\n    # Decompose to get a matrix with just scale and shear\n    no_rot = la.cholesky(no_rot).T\n\n    scale = np.diag(no_rot)\n    shear = (no_rot / scale[:, None])[np.triu_indices(len(no_rot), k=1)]\n    rot = affine @ la.inv(no_rot)\n\n    return scale, shear, rot\n</code></pre>"},{"location":"reference/transform/#megham.transform.decompose_rotation","title":"<code>decompose_rotation(rotation)</code>","text":"<p>Decompose a rotation matrix into its xyz rotation angles. This currently won't work on anything higher than 3 dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>rotation</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) rotation matrix.</p> required <p>Returns:</p> Name Type Description <code>angles</code> <code>NDArray[floating]</code> <p>The rotation angles in radians. If the input is 3d then this has 3 angles in xyz order, if 2d it just has one.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If affine is not ndim by ndim. If ndim is not 2 or 3.</p> Source code in <code>megham/transform.py</code> <pre><code>def decompose_rotation(rotation: NDArray[np.floating]) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Decompose a rotation matrix into its xyz rotation angles.\n    This currently won't work on anything higher than 3 dimensions.\n\n    Parameters\n    ----------\n    rotation : NDArray[np.floating]\n        The (ndim, ndim) rotation matrix.\n\n    Returns\n    -------\n    angles : NDArray[np.floating]\n        The rotation angles in radians.\n        If the input is 3d then this has 3 angles in xyz order,\n        if 2d it just has one.\n\n    Raises\n    ------\n    ValueError\n        If affine is not ndim by ndim.\n        If ndim is not 2 or 3.\n    \"\"\"\n    ndim = len(rotation)\n    if ndim &gt; 3:\n        raise ValueError(\"No support for rotations in more than 3 dimensions\")\n    if ndim &lt; 2:\n        raise ValueError(\"Rotations with less than 2 dimensions don't make sense\")\n    if rotation.shape != (ndim, ndim):\n        raise ValueError(\"Rotation matrix should be ndim by ndim\")\n    _rotation = np.eye(3)\n    _rotation[:ndim, :ndim] = rotation\n    angles = R.from_matrix(_rotation).as_euler(\"xyz\")\n\n    if ndim == 2:\n        angles = angles[-1:]\n    return angles\n</code></pre>"},{"location":"reference/transform/#megham.transform.decompose_transform","title":"<code>decompose_transform(transform, shift, transform_1, shift_1)</code>","text":"<p>Decompose transformations to get one with the other removed. This is solving for transform_2 and shift_2 in the following equation: dst = src@transform + shift = (src@transform_1 + shift)@transform_2 + shift_2</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>NDArray[floating]</code> <p>The composed transform (affine or rotation matrix). Should have shape (ndim, ndim).</p> required <code>shift</code> <code>NDArray[floating]</code> <p>The composed shift. Should have shape (ndim,)</p> required <code>transform_1</code> <code>NDArray[floating]</code> <p>The transform (affine or rotation matrix) to remove. Should have shape (ndim, ndim).</p> required <code>shift_1</code> <code>NDArray[floating]</code> <p>The shift to remove. Should have shape (ndim,)</p> required <p>Returns:</p> Name Type Description <code>transform_2</code> <code>NDArray[floating]</code> <p>The transform with the first transform removed. Has shape (ndim, ndim).</p> <code>shift_2</code> <code>NDArray[np.floating].</code> <p>The shift with the first transform removed. Has shape (ndim,).</p> Source code in <code>megham/transform.py</code> <pre><code>def decompose_transform(\n    transform: NDArray[np.floating],\n    shift: NDArray[np.floating],\n    transform_1: NDArray[np.floating],\n    shift_1: NDArray[np.floating],\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Decompose transformations to get one with the other removed.\n    This is solving for transform_2 and shift_2 in the following equation:\n    dst = src@transform + shift = (src@transform_1 + shift)@transform_2 + shift_2\n\n    Parameters\n    ----------\n    transform : NDArray[np.floating]\n        The composed transform (affine or rotation matrix).\n        Should have shape (ndim, ndim).\n    shift : NDArray[np.floating]\n        The composed shift.\n        Should have shape (ndim,)\n    transform_1 : NDArray[np.floating]\n        The transform (affine or rotation matrix) to remove.\n        Should have shape (ndim, ndim).\n    shift_1 : NDArray[np.floating]\n        The shift to remove.\n        Should have shape (ndim,)\n\n    Returns\n    -------\n    transform_2 : NDArray[np.floating]\n        The transform with the first transform removed.\n        Has shape (ndim, ndim).\n    shift_2 : NDArray[np.floating].\n        The shift with the first transform removed.\n        Has shape (ndim,).\n    \"\"\"\n    transform_2 = np.linalg.inv(transform_1) @ transform\n    shift_2 = shift - shift_1 @ transform_2\n\n    return transform_2, shift_2\n</code></pre>"},{"location":"reference/transform/#megham.transform.get_affine","title":"<code>get_affine(src, dst, weights=None, center_dst=True, force_svd=False, **kwargs)</code>","text":"<p>Get affine transformation between two point clouds. It is assumed that the point clouds have the same registration, ie. src[i] corresponds to dst[i].</p> <p>Transformation is dst = src@affine + shift.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of source points.</p> required <code>dst</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of destination points.</p> required <code>weights</code> <code>Optional[NDArray[floating]]</code> <p>(npoints,) array of weights to use. If provided a weighted least squares is done instead of an SVD.</p> <code>None</code> <code>center_dst</code> <code>bool</code> <p>If True, dst will be recentered at the origin before computing transformation. This is done with get_shift, but weights will not be used if provided.</p> <code>True</code> <code>force_svd</code> <code>bool</code> <p>If True the SVD is used even if there are a small number of points or weights are present.</p> <code>False</code> <code>**kwargs</code> <p>Arguments to pass to get_shift.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>affine</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) transformation matrix.</p> <code>shift</code> <code>NDArray[floating]</code> <p>The (ndim,) shift to apply after transformation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input point clouds have different shapes. If the input point clouds don't have enough points.</p> Source code in <code>megham/transform.py</code> <pre><code>def get_affine(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    weights: Optional[NDArray[np.floating]] = None,\n    center_dst: bool = True,\n    force_svd: bool = False,\n    **kwargs,\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Get affine transformation between two point clouds.\n    It is assumed that the point clouds have the same registration,\n    ie. src[i] corresponds to dst[i].\n\n    Transformation is dst = src@affine + shift.\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        A (npoints, ndim) array of source points.\n    dst : NDArray[np.floating]\n        A (npoints, ndim) array of destination points.\n    weights : Optional[NDArray[np.floating]], default: None\n        (npoints,) array of weights to use.\n        If provided a weighted least squares is done instead of an SVD.\n    center_dst : bool, default: True\n        If True, dst will be recentered at the origin before computing transformation.\n        This is done with get_shift, but weights will not be used if provided.\n    force_svd : bool, default: False\n        If True the SVD is used even if there are a small number of points\n        or weights are present.\n    **kwargs\n        Arguments to pass to get_shift.\n\n    Returns\n    -------\n    affine : NDArray[np.floating]\n        The (ndim, ndim) transformation matrix.\n    shift : NDArray[np.floating]\n        The (ndim,) shift to apply after transformation.\n\n    Raises\n    ------\n    ValueError\n        If the input point clouds have different shapes.\n        If the input point clouds don't have enough points.\n    \"\"\"\n    if src.shape != dst.shape:\n        raise ValueError(\"Input point clouds should have the same shape\")\n\n    msk = np.isfinite(src).all(axis=1) * np.isfinite(dst).all(axis=1)\n    if np.sum(msk) &lt; src.shape[1] + 1:\n        raise ValueError(\"Not enough finite points to compute transformation\")\n\n    # When we have a small number of points lstsq is better than SVD\n    # Condition is a bit arbitrary for now\n    if force_svd is False and weights is None and np.sum(msk) &lt; 50 * src.shape[1]:\n        weights = np.ones(len(src))\n\n    _dst = dst[msk].copy()\n    if center_dst:\n        _dst += get_shift(_dst, np.zeros(1), **kwargs)\n    _src = src[msk].copy()\n    init_shift = get_shift(_src, _dst, weights=weights, **kwargs)\n\n    if force_svd or weights is None:\n        M = np.vstack((_src.T, (_dst - init_shift).T)).T\n        *_, vh = la.svd(M)\n        vh_splits = [\n            quad\n            for half in np.split(vh.T, 2, axis=0)\n            for quad in np.split(half, 2, axis=1)\n        ]\n        affine = np.dot(vh_splits[2], la.pinv(vh_splits[0])).T\n        shift = init_shift\n    else:\n        rt_weight = np.sqrt(weights[msk])[..., None]\n        wsrc = rt_weight * _src\n        wdst = rt_weight * (_dst - init_shift)\n        x, *_ = la.lstsq(\n            np.column_stack((wsrc, np.ones(len(wsrc)))), wdst, check_finite=False\n        )\n        affine = x[:-1]\n        shift = x[-1] + init_shift\n\n    transformed = src[msk] @ affine + shift\n    shift += get_shift(transformed, dst[msk], **kwargs)\n\n    return affine, shift\n</code></pre>"},{"location":"reference/transform/#megham.transform.get_affine_two_stage","title":"<code>get_affine_two_stage(src, dst, weights)</code>","text":"<p>Get affine transformation between two point clouds with a two stage solver. This first uses the SVD to do an intitial alignment and then uses weighted least squares to compute a correction on top of that.</p> <p>Transformation is dst = affine@src + shift</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of source points.</p> required <code>dst</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of destination points.</p> required <code>weights</code> <code>NDArray[floating]</code> <p>(npoints,) array of weights to use. If provided a weighted least squares is done instead of an SVD.</p> required <p>Returns:</p> Name Type Description <code>affine</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) transformation matrix.</p> <code>shift</code> <code>NDArray[floating]</code> <p>The (ndim,) shift to apply after transformation.</p> Source code in <code>megham/transform.py</code> <pre><code>def get_affine_two_stage(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    weights: NDArray[np.floating],\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Get affine transformation between two point clouds with a two stage solver.\n    This first uses the SVD to do an intitial alignment and\n    then uses weighted least squares to compute a correction on top of that.\n\n    Transformation is dst = affine@src + shift\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        A (npoints, ndim) array of source points.\n    dst : NDArray[np.floating]\n        A (npoints, ndim) array of destination points.\n    weights : NDArray[np.floating]\n        (npoints,) array of weights to use.\n        If provided a weighted least squares is done instead of an SVD.\n\n    Returns\n    -------\n    affine : NDArray[np.floating]\n        The (ndim, ndim) transformation matrix.\n    shift : NDArray[np.floating]\n        The (ndim,) shift to apply after transformation.\n    \"\"\"\n    # Do an initial alignment without weights\n    affine_0, shift_0 = get_affine(src, dst, force_svd=True)\n    init_align = apply_transform(src, affine_0, shift_0)\n    # Now compute the actual transform\n    affine, shift = get_affine(init_align, dst, weights)\n    # Compose the transforms\n    affine, shift = compose_transform(affine_0, shift_0, affine, shift)\n    # Now one last shift correction\n    transformed = apply_transform(src, affine, shift)\n    shift += get_shift(transformed, dst, \"mean\", weights)\n\n    return affine, shift\n</code></pre>"},{"location":"reference/transform/#megham.transform.get_rigid","title":"<code>get_rigid(src, dst, center_dst=True, **kwargs)</code>","text":"<p>Get rigid transformation between two point clouds. It is assumed that the point clouds have the same registration, ie. src[i] corresponds to dst[i].</p> <p>Transformation is dst = src@rot + shift.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of source points.</p> required <code>dst</code> <code>NDArray[floating]</code> <p>A (npoints, ndim) array of destination points.</p> required <code>center_dst</code> <code>bool</code> <p>If True, dst will be recentered at the origin before computing transformation. This is done with get_shift, but weights will not be used if provided.</p> <code>True</code> <code>**kwargs</code> <p>Arguments to pass to get_shift.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>rotation</code> <code>NDArray[floating]</code> <p>The (ndim, ndim) rotation matrix.</p> <code>shift</code> <code>NDArray[floating]</code> <p>The (ndim,) shift to apply after transformation. If point are in col basis will be returned as a column vector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input point clouds have different shapes. If the input point clouds don't have enough points.</p> Source code in <code>megham/transform.py</code> <pre><code>def get_rigid(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    center_dst: bool = True,\n    **kwargs,\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Get rigid transformation between two point clouds.\n    It is assumed that the point clouds have the same registration,\n    ie. src[i] corresponds to dst[i].\n\n    Transformation is dst = src@rot + shift.\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        A (npoints, ndim) array of source points.\n    dst : NDArray[np.floating]\n        A (npoints, ndim) array of destination points.\n    center_dst : bool, default: True\n        If True, dst will be recentered at the origin before computing transformation.\n        This is done with get_shift, but weights will not be used if provided.\n    **kwargs\n        Arguments to pass to get_shift.\n\n    Returns\n    -------\n    rotation : NDArray[np.floating]\n        The (ndim, ndim) rotation matrix.\n    shift : NDArray[np.floating]\n        The (ndim,) shift to apply after transformation.\n        If point are in col basis will be returned as a column vector.\n\n    Raises\n    ------\n    ValueError\n        If the input point clouds have different shapes.\n        If the input point clouds don't have enough points.\n    \"\"\"\n    if src.shape != dst.shape:\n        raise ValueError(\"Input point clouds should have the same shape\")\n\n    msk = np.isfinite(src).all(axis=1) * np.isfinite(dst).all(axis=1)\n    ndim = src.shape[1]\n    if np.sum(msk) &lt; ndim * (ndim - 1) / 2:\n        raise ValueError(\"Not enough finite points to compute transformation\")\n\n    _dst = dst[msk].copy()\n    if center_dst:\n        _kwargs = kwargs.copy()\n        _kwargs.update({\"weights\": None})\n        _dst += get_shift(_dst, np.zeros(1), **_kwargs)\n    _src = src[msk].copy()\n    _src += get_shift(_src, _dst, **kwargs)\n\n    M = _src.T @ (_dst)\n    u, _, vh = la.svd(M)\n    v = vh.T\n    uT = u.T\n\n    corr = np.eye(ndim)\n    corr[-1, -1] = la.det((v) @ (uT))\n    rot = v @ corr @ uT\n    rot = rot.T\n\n    transformed = src[msk] @ rot\n    shift = get_shift(transformed, dst[msk], **kwargs)\n\n    return rot, shift\n</code></pre>"},{"location":"reference/transform/#megham.transform.get_shift","title":"<code>get_shift(src, dst, method='median', weights=None)</code>","text":"<p>Get shift between two point clouds. Shift can be applied as dst = src + shift.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>A (ndim, npoints) array of source points.</p> required <code>dst</code> <code>NDArray[floating]</code> <p>Nominally a (ndim, npoints) array of destination points, but really any array broadcastable with src is accepted. Some useful options are: * np.zeros(1) to align with the origin * A (ndim,) array to align with an arbitrary point</p> required <code>method</code> <code>str</code> <p>Method to use to align points. Current accepted values are: 'median' and 'mean'</p> <code>'median'</code> <code>weights</code> <code>Optional[NDArray[floating]]</code> <p>(npoints,) array of weights to use. If provided and method is 'mean' then a weighted average is used. If method is median this is not currently used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>shift</code> <code>NDArray[floating]</code> <p>The (ndim,) shift to apply after transformation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid method is provided</p> Source code in <code>megham/transform.py</code> <pre><code>def get_shift(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    method: str = \"median\",\n    weights: Optional[NDArray[np.floating]] = None,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Get shift between two point clouds.\n    Shift can be applied as dst = src + shift.\n\n    Parameters\n    -----------\n    src : NDArray[np.floating]\n        A (ndim, npoints) array of source points.\n    dst : NDArray[np.floating]\n        Nominally a (ndim, npoints) array of destination points,\n        but really any array broadcastable with src is accepted.\n        Some useful options are:\n        * np.zeros(1) to align with the origin\n        * A (ndim,) array to align with an arbitrary point\n    method : str, default: 'median'\n        Method to use to align points.\n        Current accepted values are: 'median' and 'mean'\n    weights : Optional[NDArray[np.floating]], default: None\n        (npoints,) array of weights to use.\n        If provided and method is 'mean' then a weighted average is used.\n        If method is median this is not currently used.\n\n    Returns\n    -------\n    shift : NDArray[np.floating]\n        The (ndim,) shift to apply after transformation.\n\n    Raises\n    ------\n    ValueError\n        If an invalid method is provided\n    \"\"\"\n    if method not in [\"median\", \"mean\"]:\n        raise ValueError(f\"Invalid method: {method}\")\n\n    shift = np.zeros(src.shape[1])\n    if method == \"median\":\n        shift = np.median(dst - src, axis=0)\n    elif method == \"mean\":\n        if weights is None:\n            shift = np.mean(dst - src, axis=0)\n        else:\n            wdiff = weights[..., None] * (dst - src)\n            shift = np.nansum(wdiff, axis=0) / np.nansum(weights)\n\n    return shift\n</code></pre>"},{"location":"reference/transform/#megham.transform.invert_transform","title":"<code>invert_transform(transform, shift)</code>","text":"<p>Invert a transformation. If the inverted transformation is applied to a point cloud that has already been transformed, you will recover the original point cloud.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>NDArray[floating]</code> <p>The transform (affine or rotation matrix) the invert. Should have shape (ndim, ndim).</p> required <code>shift</code> <code>NDArray[floating]</code> <p>The shift to invert. Should have shape (ndim,)</p> required <p>Returns:</p> Name Type Description <code>transform_inv</code> <code>NDArray[floating]</code> <p>The inverted transformation matrix.</p> <code>shift_inv</code> <code>NDArray[floating]</code> <p>The inverted shift vector.</p> Source code in <code>megham/transform.py</code> <pre><code>def invert_transform(\n    transform: NDArray[np.floating], shift: NDArray[np.floating]\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating]]:\n    \"\"\"\n    Invert a transformation.\n    If the inverted transformation is applied to a point cloud that has already been\n    transformed, you will recover the original point cloud.\n\n    Parameters\n    ----------\n    transform : NDArray[np.floating]\n        The transform (affine or rotation matrix) the invert.\n        Should have shape (ndim, ndim).\n    shift : NDArray[np.floating]\n        The shift to invert.\n        Should have shape (ndim,)\n\n    Returns\n    -------\n    transform_inv : NDArray[np.floating]\n        The inverted transformation matrix.\n    shift_inv : NDArray[np.floating]\n        The inverted shift vector.\n    \"\"\"\n    transform_inv = np.linalg.inv(transform)\n    shift_inv = (-1 * shift) @ transform_inv\n\n    return transform_inv, shift_inv\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#megham.utils.estimate_spacing","title":"<code>estimate_spacing(coords)</code>","text":"<p>Estimate the spacing between points in a point cloud. This is just the median distance between nearest neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>NDArray[floating]</code> <p>The point cloud to estimate spacing of. Should have shape (npoint, ndim).</p> required <p>Returns:</p> Name Type Description <code>spacing</code> <code>float</code> <p>The spacing between points.</p> Source code in <code>megham/utils.py</code> <pre><code>def estimate_spacing(coords: NDArray[np.floating]) -&gt; float:\n    \"\"\"\n    Estimate the spacing between points in a point cloud.\n    This is just the median distance between nearest neighbors.\n\n    Parameters\n    ----------\n    coords : NDArray[np.floating]\n        The point cloud to estimate spacing of.\n        Should have shape (npoint, ndim).\n\n    Returns\n    -------\n    spacing : float\n        The spacing between points.\n    \"\"\"\n    edm = make_edm(coords)\n    edm[edm == 0] = np.nan\n    nearest_dists = np.nanmin(edm, axis=0)\n\n    return np.median(nearest_dists)\n</code></pre>"},{"location":"reference/utils/#megham.utils.estimate_var","title":"<code>estimate_var(src, dst, dim_groups=None)</code>","text":"<p>Estimate variance between point clouds for use with something like a GMM.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>The set of source points to be mapped onto the target points. Should have shape (nsrcpoints, ndim).</p> required <code>dst</code> <code>NDArray[floating]</code> <p>The set of destination points to be mapped onto. Should have shape (ndstpoints, ndim).</p> required <code>dim_groups</code> <code>Optional[Sequence[Sequence[int] | NDArray[int_]]]</code> <p>Which dimensions should be computed together. If None all dimensions will be treated seperately.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>var</code> <code>NDArray[floating]</code> <p>The estimated variance. Will have shape (ndim,).</p> Source code in <code>megham/utils.py</code> <pre><code>def estimate_var(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    dim_groups: Optional[Sequence[Sequence[int] | NDArray[np.int_]]] = None,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Estimate variance between point clouds for use with something like a GMM.\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        The set of source points to be mapped onto the target points.\n        Should have shape (nsrcpoints, ndim).\n    dst : NDArray[np.floating]\n        The set of destination points to be mapped onto.\n        Should have shape (ndstpoints, ndim).\n    dim_groups : Optional[Sequence[Sequence[int] | NDArray[np.int_]]], default: None\n        Which dimensions should be computed together.\n        If None all dimensions will be treated seperately.\n\n    Returns\n    -------\n    var : NDArray[np.floating]\n        The estimated variance.\n        Will have shape (ndim,).\n    \"\"\"\n    nsrcpoints, ndim = src.shape\n    ndstpoints = len(dst)\n\n    if dim_groups is None:\n        dim_groups = [[dim] for dim in range(ndim)]\n    else:\n        dims_flat = np.concatenate(dim_groups)\n        no_group = np.setdiff1d(np.arange(ndim), dims_flat)\n        dim_groups = list(dim_groups)\n        dim_groups = dim_groups + [[dim] for dim in no_group]\n\n    var = np.zeros(ndim)\n    for dim_group in dim_groups:\n        sq_diff = dist.cdist(src[:, dim_group], dst[:, dim_group], metric=\"sqeuclidean\")\n        var[dim_group] = np.nansum(sq_diff) / (len(dim_group) * nsrcpoints * ndstpoints)\n\n    return var\n</code></pre>"},{"location":"reference/utils/#megham.utils.gen_weights","title":"<code>gen_weights(src, dst, var=None, pdf=False)</code>","text":"<p>Generate weights between points in two registered point clouds. The weight here is just the liklihood from a gaussian. Note that this is not a GMM, each weight is computed from a single gaussian since we are assuming a known registration.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>NDArray[floating]</code> <p>The set of source points to be mapped onto the target points. Should have shape (nsrcpoints, ndim).</p> required <code>dst</code> <code>NDArray[floating]</code> <p>The set of destination points to be mapped onto. Should have shape (ndstpoints, ndim).</p> required <code>var</code> <code>Optional[NDArray[floating]]</code> <p>The variance along each axis. Should have shape (ndim,) if provided. If None, will be computed with estimate_var</p> <code>None</code> <code>pdf</code> <code>bool</code> <p>If True apply the 1/sqrt(2pivar) normalization factor. This makes the weights the PDF of a normal distribution.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>NDArray[floating]</code> <p>(npoints,) array of weights.</p> Source code in <code>megham/utils.py</code> <pre><code>def gen_weights(\n    src: NDArray[np.floating],\n    dst: NDArray[np.floating],\n    var: Optional[NDArray[np.floating]] = None,\n    pdf: bool = False,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Generate weights between points in two registered point clouds.\n    The weight here is just the liklihood from a gaussian.\n    Note that this is not a GMM, each weight is computed from a single\n    gaussian since we are assuming a known registration.\n\n    Parameters\n    ----------\n    src : NDArray[np.floating]\n        The set of source points to be mapped onto the target points.\n        Should have shape (nsrcpoints, ndim).\n    dst : NDArray[np.floating]\n        The set of destination points to be mapped onto.\n        Should have shape (ndstpoints, ndim).\n    var : Optional[NDArray[np.floating]], default: None\n        The variance along each axis.\n        Should have shape (ndim,) if provided.\n        If None, will be computed with estimate_var\n    pdf : bool, default: False\n        If True apply the 1/sqrt(2*pi*var) normalization factor.\n        This makes the weights the PDF of a normal distribution.\n\n    Returns\n    -------\n    weights : NDArray[np.floating]\n        (npoints,) array of weights.\n    \"\"\"\n    if var is None:\n        var = estimate_var(src, dst)\n    norm = np.ones_like(var)\n    if pdf:\n        norm = 1.0 / np.sqrt(2 * np.pi * var)\n\n    # Compute nd gaussian for each pair\n    weights = np.prod(norm * np.exp(-0.5 * (src - dst) ** 2 / var), axis=1)\n\n    return weights\n</code></pre>"},{"location":"reference/utils/#megham.utils.make_edm","title":"<code>make_edm(coords)</code>","text":"<p>Make an Euclidean distance matrix from a set of points.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>NDArray[floating]</code> <p>The (npoint, ndim) array of input points.</p> required <p>Returns:</p> Name Type Description <code>edm</code> <code>NDArray[floating]</code> <p>The (npoint, npoint) euclidean distance matrix.</p> Source code in <code>megham/utils.py</code> <pre><code>def make_edm(coords: NDArray[np.floating]) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Make an Euclidean distance matrix from a set of points.\n\n    Parameters\n    ----------\n    coords : NDArray[np.floating]\n        The (npoint, ndim) array of input points.\n\n    Returns\n    -------\n    edm : NDArray[np.floating]\n        The (npoint, npoint) euclidean distance matrix.\n    \"\"\"\n    dist_vec = dist.pdist(coords)\n    edm = dist.squareform(dist_vec)\n\n    return edm\n</code></pre>"},{"location":"reference/registration/cpd/","title":"cpd","text":"<p>Module for performing coherent point drift</p>"},{"location":"reference/registration/cpd/#megham.registration.cpd.Callback","title":"<code>Callback</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Callback idea shamelessly stolen from pycpd.</p> Source code in <code>megham/registration/cpd.py</code> <pre><code>class Callback(Protocol):\n    \"\"\"\n    Callback idea shamelessly stolen from pycpd.\n    \"\"\"\n\n    def __call__(\n        self,\n        target: NDArray[np.floating],\n        transformed: NDArray[np.floating],\n        iteration: int,\n        err: float,\n    ):\n        ...\n</code></pre>"},{"location":"reference/registration/cpd/#megham.registration.cpd.compute_P","title":"<code>compute_P(source, target, var, w)</code>","text":"<p>Compute matrix of probabilities of matches between points in source and target.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>NDArray[floating]</code> <p>The set of source points to be mapped onto the target points. Nominally when you run this it will be the source points transformed to line up with target. Should have shape (nsrcpoints, ndim).</p> required <code>target</code> <code>NDArray[floating]</code> <p>The set of target points to be mapped onto. Should have shape (ntrgpoints, ndim).</p> required <code>var</code> <code>NDArray[floating]</code> <p>The variance of the gaussian mixture model. Should have shape (ndim,).</p> required <code>w</code> <code>float</code> <p>The weight of the uniform distrubution.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>P</code> <code>NDArray[floating]</code> <p>Probability matrix of matches between the source and target points. P[i,j] is the probability that source[i] corresponds to target[j]. Has shape (nsrcpoints, ntrgpoints).</p> Source code in <code>megham/registration/cpd.py</code> <pre><code>def compute_P(\n    source: NDArray[np.floating],\n    target: NDArray[np.floating],\n    var: NDArray[np.floating],\n    w: float,\n) -&gt; NDArray[np.floating]:\n    \"\"\"\n    Compute matrix of probabilities of matches between points in source and target.\n\n    Parameters\n    ----------\n    source : NDArray[np.floating]\n        The set of source points to be mapped onto the target points.\n        Nominally when you run this it will be the source points transformed to line up with target.\n        Should have shape (nsrcpoints, ndim).\n    target : NDArray[np.floating]\n        The set of target points to be mapped onto.\n        Should have shape (ntrgpoints, ndim).\n    var : NDArray[np.floating]\n        The variance of the gaussian mixture model.\n        Should have shape (ndim,).\n    w : float, default: 0.0\n        The weight of the uniform distrubution.\n\n    Returns\n    -------\n    P : NDArray[np.floating]\n        Probability matrix of matches between the source and target points.\n        P[i,j] is the probability that source[i] corresponds to target[j].\n        Has shape (nsrcpoints, ntrgpoints).\n    \"\"\"\n    # TODO: implement fast gaussian transform\n    nsrcpoints, ndim = source.shape\n    ntrgpoints = len(target)\n\n    uni = (\n        (w / (1 - w))\n        * (nsrcpoints / ntrgpoints)\n        * np.sqrt(((2 * np.pi) ** ndim) * np.product(var))\n    )\n    gaussians = np.ones((nsrcpoints, ntrgpoints))\n    for dim in range(ndim):\n        sq_diff = dist.cdist(\n            np.atleast_2d(source[:, dim]).T,\n            np.atleast_2d(target[:, dim]).T,\n            metric=\"sqeuclidean\",\n        )\n        gaussians *= np.exp(-0.5 * sq_diff / var[dim])\n    norm_fac = np.clip(np.sum(gaussians, axis=0), epsilon, None)\n\n    P = gaussians / (norm_fac + uni)\n\n    return P\n</code></pre>"},{"location":"reference/registration/cpd/#megham.registration.cpd.cpd","title":"<code>cpd(source, target, w=0.0, eps=1e-10, max_iters=500, callback=dummy_callback, method='affine')</code>","text":"<p>Compute the CPD. This is just a wrapper around joint_cpd that puts everything into one dim_group. See the docstring of joint_cpd for details on the parameters and returns.</p> <p>See https://arxiv.org/abs/0905.2635 for details on the base CPD algorithm.</p> Source code in <code>megham/registration/cpd.py</code> <pre><code>def cpd(\n    source: NDArray[np.floating],\n    target: NDArray[np.floating],\n    w: float = 0.0,\n    eps: float = 1e-10,\n    max_iters: int = 500,\n    callback: Callback = dummy_callback,\n    method: str = \"affine\",\n) -&gt; tuple[\n    NDArray[np.floating],\n    NDArray[np.floating],\n    NDArray[np.floating],\n    NDArray[np.floating],\n]:\n    \"\"\"\n    Compute the CPD.\n    This is just a wrapper around joint_cpd that puts everything into one dim_group.\n    See the docstring of joint_cpd for details on the parameters and returns.\n\n    See https://arxiv.org/abs/0905.2635 for details on the base CPD algorithm.\n    \"\"\"\n    return joint_cpd(\n        source=source,\n        target=target,\n        dim_groups=None,\n        w=w,\n        eps=eps,\n        max_iters=max_iters,\n        callback=callback,\n        method=method,\n    )\n</code></pre>"},{"location":"reference/registration/cpd/#megham.registration.cpd.joint_cpd","title":"<code>joint_cpd(source, target, dim_groups=None, w=0.0, eps=0.001, max_iters=500, callback=dummy_callback, method='affine')</code>","text":"<p>Compute the joint CPD.</p> <p>This is an extension of the base CPD algorithm that allows you to fit for transforms that treat groups of dimensions seperately but use a jointly computed probability matrix and variance. This is useful in cases where you have information about a set of points that is correlated but in a different basis so you want to avoid degrees of freedom that mix the disprate baseis. For example if you have a set of points where you have both spatial information as well as some non-spatial scalar associated with each point (ie: frequency) you can use this to compute a registration that considers these jointly but transforms them seperately to avoid a scenario where you have a rotation between a spatial and a non-spatial dimension.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>NDArray[floating]</code> <p>The set of source points to be mapped onto the target points. Should have shape (nsrcpoints, ndim).</p> required <code>target</code> <code>NDArray[floating]</code> <p>The set of target points to be mapped onto. Should have shape (ntrgpoints, ndim).</p> required <code>dim_groups</code> <code>Optional[Sequence[Sequence[int] | NDArray[int_]]]</code> <p>Which dimensions should be transformed together. Each element in this sequence should be a sequence of array of ints that correspond to the columns of source and target that are transformed together. Any columns that are not included here will not be transformed but will still be used when computing the probability and variance. None of the elements in this sequence can have overlap. If set to None all the dimensions will be transformed together.</p> <code>None</code> <code>w</code> <code>float</code> <p>The weight of the uniform distrubution. Set higher to reduce sensitivity to noise and outliers at the expense of potentially worse performance. Should be in the range [0, 1), if not it will snap to one of the bounds.</p> <code>0.0</code> <code>eps</code> <code>float</code> <p>The convergence criteria. When the change in the objective function is less than or equal to this we stop.</p> <code>1e-10</code> <code>max_iters</code> <code>int</code> <p>The maximum number of iterations to run for.</p> <code>500</code> <code>callback</code> <code>Callback</code> <p>Function that runs once per iteration, can be used to visualize the match process. See the Callback Protocol for details on the expected signature.</p> <code>dummy_callback</code> <code>method</code> <code>str</code> <p>The type of transformation to compute. Acceptable values are: affine, rigid. If any other value is passed then transform will be the identity.</p> <code>'affine'</code> <p>Returns:</p> Name Type Description <code>transform</code> <code>NDArray[floating]</code> <p>The transform transformation that takes source to target. Apply using megham.transform.apply_transform. Has shape (ndim, ndim).</p> <code>shift</code> <code>NDArray[floating]</code> <p>The transformation that takes source to target after transform is applied. Apply using megham.transform.apply_transform. Has shape (ndim,).</p> <code>transformed</code> <code>NDArray[floating]</code> <p>Source transformed to align with target. Has shape (nsrcpoints, ndim).</p> <code>P</code> <code>NDArray[floating]</code> <p>Probability matrix of matches between the source and target points. P[i,j] is the probability that source[i] corresponds to target[j]. Has shape (nsrcpoints, ntrgpoints).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source and target don't share ndim. If dim_groups has repeated dimensions or invalid dimensions.</p> Source code in <code>megham/registration/cpd.py</code> <pre><code>def joint_cpd(\n    source: NDArray[np.floating],\n    target: NDArray[np.floating],\n    dim_groups: Optional[Sequence[Sequence[int] | NDArray[np.int_]]] = None,\n    w: float = 0.0,\n    eps: float = 1e-3,\n    max_iters: int = 500,\n    callback: Callback = dummy_callback,\n    method: str = \"affine\",\n) -&gt; tuple[\n    NDArray[np.floating],\n    NDArray[np.floating],\n    NDArray[np.floating],\n    NDArray[np.floating],\n]:\n    \"\"\"\n    Compute the joint CPD.\n\n    This is an extension of the base CPD algorithm that allows you to fit for\n    transforms that treat groups of dimensions seperately but use a jointly computed\n    probability matrix and variance.\n    This is useful in cases where you have information about a set of points that is correlated\n    but in a different basis so you want to avoid degrees of freedom that mix the disprate baseis.\n    For example if you have a set of points where you have both spatial information as well as some\n    non-spatial scalar associated with each point (ie: frequency) you can use this to compute a\n    registration that considers these jointly but transforms them seperately to avoid a scenario\n    where you have a rotation between a spatial and a non-spatial dimension.\n\n    Parameters\n    ----------\n    source : NDArray[np.floating]\n        The set of source points to be mapped onto the target points.\n        Should have shape (nsrcpoints, ndim).\n    target : NDArray[np.floating]\n        The set of target points to be mapped onto.\n        Should have shape (ntrgpoints, ndim).\n    dim_groups : Optional[Sequence[Sequence[int] | NDArray[np.int_]]], default: None\n        Which dimensions should be transformed together.\n        Each element in this sequence should be a sequence of array of ints that correspond to the\n        columns of source and target that are transformed together.\n        Any columns that are not included here will not be transformed but will still be used when\n        computing the probability and variance.\n        None of the elements in this sequence can have overlap.\n        If set to None all the dimensions will be transformed together.\n    w : float, default: 0.0\n        The weight of the uniform distrubution.\n        Set higher to reduce sensitivity to noise and outliers at the expense\n        of potentially worse performance.\n        Should be in the range [0, 1), if not it will snap to one of the bounds.\n    eps : float, default: 1e-10\n        The convergence criteria.\n        When the change in the objective function is less than or equal to this we stop.\n    max_iters : int, default: 500\n        The maximum number of iterations to run for.\n    callback: Callback, default: dummy_callback\n        Function that runs once per iteration, can be used to visualize the match process.\n        See the Callback Protocol for details on the expected signature.\n    method : str, default: 'affine'\n        The type of transformation to compute.\n        Acceptable values are: affine, rigid.\n        If any other value is passed then transform will be the identity.\n\n    Returns\n    -------\n    transform : NDArray[np.floating]\n        The transform transformation that takes source to target.\n        Apply using megham.transform.apply_transform.\n        Has shape (ndim, ndim).\n    shift : NDArray[np.floating]\n        The transformation that takes source to target after transform is applied.\n        Apply using megham.transform.apply_transform.\n        Has shape (ndim,).\n    transformed : NDArray[np.floating]\n        Source transformed to align with target.\n        Has shape (nsrcpoints, ndim).\n    P : NDArray[np.floating]\n        Probability matrix of matches between the source and target points.\n        P[i,j] is the probability that source[i] corresponds to target[j].\n        Has shape (nsrcpoints, ntrgpoints).\n\n    Raises\n    ------\n    ValueError\n        If source and target don't share ndim.\n        If dim_groups has repeated dimensions or invalid dimensions.\n    \"\"\"\n    ndim = source.shape[1]\n    if target.shape[1] != ndim:\n        raise ValueError(\n            f\"Source and target don't have same ndim ({ndim} vs {target.shape[1]})\"\n        )\n    if dim_groups is None:\n        dim_groups = [\n            np.arange(ndim),\n        ]\n    else:\n        dims_flat = np.concatenate(dim_groups)\n        dims_bad = dims_flat[(dims_flat &lt; 0) + (dims_flat &gt;= ndim)]\n        if len(dims_bad):\n            raise ValueError(f\"Invalid dimensions in dim_groups: {dims_bad}\")\n        dims_uniq, counts = np.unique(dims_flat, return_counts=True)\n        repeats = dims_uniq[counts &gt; 1]\n        if len(repeats):\n            raise ValueError(f\"Repeated dimensions in dim_groups: {repeats}\")\n        dim_groups = [np.array(dim_group, dtype=int) for dim_group in dim_groups]\n\n    var = estimate_var(source, target, dim_groups)\n    err = np.inf\n\n    transform = np.eye(ndim)\n    shift = np.zeros(ndim)\n\n    transformed = source.copy()\n    P = np.ones((len(source), len(target)))\n    for i in range(max_iters):\n        _err, _transform, _shift = err, transform, shift\n        transformed = apply_transform(source, transform, shift)\n        P = compute_P(transformed, target, var, w)\n        transform, shift, var, err = solve_transform(\n            source, target, P, dim_groups, var, method\n        )\n        callback(target, transformed, i, err)\n\n        if _err - err &lt; eps:\n            if err &gt; _err:\n                transform, shift = _transform, _shift\n            break\n\n    return transform, shift, transformed, P\n</code></pre>"},{"location":"reference/registration/cpd/#megham.registration.cpd.solve_transform","title":"<code>solve_transform(source, target, P, dim_groups, cur_var, method='affine')</code>","text":"<p>Solve for the transformation at each iteration of CPD.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>NDArray[floating]</code> <p>The set of source points to be mapped onto the target points. Should have shape (nsrcpoints, ndim).</p> required <code>target</code> <code>NDArray[floating]</code> <p>The set of target points to be mapped onto. Should have shape (ntrgpoints, ndim).</p> required <code>P</code> <code>NDArray[floating]</code> <p>Probability matrix of matches between the source and target points.</p> required <code>dim_groups</code> <code>Sequence[NDArray[int_]]</code> <p>Which dimensions should be transformed together.</p> required <code>cur_var</code> <code>float</code> <p>The current mixture model variance.</p> required <code>method</code> <code>str</code> <p>The type of transformation to compute. Acceptable values are: affine, rigid. If any other value is passed then transform will be the identity.</p> <code>'affine'</code> <p>Returns:</p> Name Type Description <code>transform</code> <code>NDArray[floating]</code> <p>Transformation between source and target.</p> <code>shift</code> <code>NDArray[floating]</code> <p>Shift to be applied after the affine transformation.</p> <code>var</code> <code>NDArray[floating]</code> <p>Current variance of the mixture model.</p> <code>err</code> <code>float</code> <p>Current value of the error function.</p> Source code in <code>megham/registration/cpd.py</code> <pre><code>def solve_transform(\n    source: NDArray[np.floating],\n    target: NDArray[np.floating],\n    P: NDArray[np.floating],\n    dim_groups: Sequence[NDArray[np.int_]],\n    cur_var: NDArray[np.floating],\n    method: str = \"affine\",\n) -&gt; tuple[NDArray[np.floating], NDArray[np.floating], NDArray[np.floating], float]:\n    \"\"\"\n    Solve for the transformation at each iteration of CPD.\n\n    Parameters\n    ----------\n    source : NDArray[np.floating]\n        The set of source points to be mapped onto the target points.\n        Should have shape (nsrcpoints, ndim).\n    target : NDArray[np.floating]\n        The set of target points to be mapped onto.\n        Should have shape (ntrgpoints, ndim).\n    P : NDArray[np.floating]\n        Probability matrix of matches between the source and target points.\n    dim_groups : Sequence[NDArray[np.int_]]\n        Which dimensions should be transformed together.\n    cur_var : float\n        The current mixture model variance.\n    method : str, default: 'affine'\n        The type of transformation to compute.\n        Acceptable values are: affine, rigid.\n        If any other value is passed then transform will be the identity.\n\n    Returns\n    -------\n    transform : NDArray[np.floating]\n        Transformation between source and target.\n    shift : NDArray[np.floating]\n        Shift to be applied after the affine transformation.\n    var : NDArray[np.floating]\n        Current variance of the mixture model.\n    err : float\n        Current value of the error function.\n    \"\"\"\n    ndim = source.shape[1]\n    N_P = np.sum(P)\n    PT1 = np.diag(np.sum(P, axis=0))\n    P1 = np.diag(np.sum(P, axis=1))\n\n    mu_src = np.sum(P.T @ source, axis=0) / N_P\n    mu_trg = np.sum(P @ target, axis=0) / N_P\n\n    src_hat = source - mu_src\n    trg_hat = target - mu_trg\n\n    transform = np.eye(ndim)\n    shift = np.zeros(ndim)\n    new_var = cur_var.copy()\n    err = 0\n    for dim_group in dim_groups:\n        mu_s = mu_src[dim_group]\n        mu_t = mu_trg[dim_group]\n        src = src_hat[:, dim_group]\n        trg = trg_hat[:, dim_group]\n\n        all_mul = trg.T @ P.T @ src\n        src_mul = src.T @ P1 @ src\n\n        if method == \"affine\":\n            tfm = np.linalg.solve(src_mul.T, all_mul.T)\n        elif method == \"rigid\":\n            U, _, V = la.svd(all_mul, full_matrices=True)\n            corr = np.eye(len(dim_group))\n            corr[-1, -1] = la.det((V) @ (U))\n            tfm = U @ corr @ V\n        else:\n            tfm = np.eye(ndim)\n\n        sft = mu_t.T - tfm.T @ mu_s.T\n\n        transform[dim_group[:, np.newaxis], dim_group] = tfm\n        shift[dim_group] = sft\n\n        trc_trf_mul = np.trace(tfm @ src_mul @ tfm)\n        trc_trg_mul = np.trace(trg.T @ PT1 @ trg)\n        trc_all = np.trace(all_mul @ tfm)\n\n        var = (trc_trg_mul - trc_all) / (N_P * len(dim_group))\n        if var &lt;= 0:\n            var = cur_var[dim_group] / 2\n        new_var[dim_group] = var\n\n        err += (trc_trg_mul - 2 * trc_all + trc_trf_mul) / (\n            2 * cur_var[dim_group][0]\n        ) + 0.5 * N_P * len(dim_group) * np.log(cur_var[dim_group][0])\n\n    return transform, shift, new_var, err\n</code></pre>"}]}